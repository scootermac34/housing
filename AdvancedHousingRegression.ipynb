{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:55:07.301626Z","iopub.execute_input":"2024-07-09T13:55:07.301990Z","iopub.status.idle":"2024-07-09T13:55:07.711358Z","shell.execute_reply.started":"2024-07-09T13:55:07.301962Z","shell.execute_reply":"2024-07-09T13:55:07.710294Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Project","metadata":{}},{"cell_type":"markdown","source":"### Summary","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Package List","metadata":{}},{"cell_type":"code","source":"#import pkg_resources\n#installed_packages = pkg_resources.working_set\n#installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n#installed_packages_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Additional Packages\nimport io","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:55:31.368174Z","iopub.execute_input":"2024-07-09T13:55:31.368675Z","iopub.status.idle":"2024-07-09T13:55:31.373358Z","shell.execute_reply.started":"2024-07-09T13:55:31.368637Z","shell.execute_reply":"2024-07-09T13:55:31.372208Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n#df_description = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\", delimiter=' ')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:55:55.462321Z","iopub.execute_input":"2024-07-09T13:55:55.462722Z","iopub.status.idle":"2024-07-09T13:55:55.509771Z","shell.execute_reply.started":"2024-07-09T13:55:55.462692Z","shell.execute_reply":"2024-07-09T13:55:55.508675Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Create Functions for EDA","metadata":{}},{"cell_type":"code","source":"# Code to determine dtypes, missing values, etc...\ndef analysis(read):\n    \n    if len(read) > 0:\n        print(\"PROCESS HAS BEEN STARTED\\n\")\n\n        print(\"DATA SHAPE\")\n        print(\"Observation:\", read.shape[0], \"Column:\", read.shape[1], \"\\n\")\n\n        print(\"EXPLORE MORE ABOUT THE DATA\")\n        if len(read.select_dtypes(\"object\").columns) > 0:\n            print(\"Object Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"object\").columns), \"\\n\", \n                  read.select_dtypes(\"object\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"integer\").columns) > 0:\n            print(\"Integer Variables:\", \"\\n\", \"VVariables:\", \n                  len(read.select_dtypes(\"integer\").columns), \"\\n\", \n                  read.select_dtypes(\"integer\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"float\").columns) > 0:\n            print(\"Float Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"float\").columns), \"\\n\", \n                  read.select_dtypes(\"float\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"bool\").columns) > 0:\n            print(\"Bool Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"bool\").columns), \"\\n\", \n                  read.select_dtypes(\"bool\").columns.tolist(), \"\\n\")\n\n        print(\"IS THERE ANY MISSING VALUE\")\n        print(\" \\n \", np.where(read.isnull().values.any() == False,\"No missing value!\", \"Data includes missing value!\"), \"\\n\")\n\n        buf = io.StringIO()\n        read.info(buf=buf)\n        check = True\n        check = buf.getvalue().split('\\n')[-2].split(\":\")[1].strip()\n        print(\"MEMORY \\n\", check)\n\n    else:\n        print(\"ERROR!\")\n\n    return read\n\ndef data_cleaning(df):\n\n    print(\"*********{} *********\".format('Inspecting missing values'))\n    \n    data = df.isna().sum().reset_index().sort_values(by=0, ascending=False)\n    clean_data = data[data[0] != 0].shape[0]\n    columns = df.shape[1]\n    rows = df.shape[0]\n    data.columns = [\"name\", \"missing appearences\"]\n    data[\"%missing from total\"] = data[data[\"missing appearences\"]!=0][\"missing appearences\"]/rows\n    mis_data = data[data[\"%missing from total\"] > 0.5].shape[0]\n    #drop_data = np.array(data[data[\"%missing from total\"] > 0.5][\"name\"])\n    \n    print(\"{}/{} total missing data in terms of column shape.\".format(clean_data, columns))\n    #print(\"{}/{} columns  will be dropped. name of the drop column is {}\".format(mis_data, columns,drop_data))\n    \n    return data#, drop_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Univariate Analysis\n## Compare Train/Test Splits\ndef univariate_eda(data, target, var) -> dict:\n    if data[var].dtype == 'object':\n        data[var] = np.where(data[var].isna(), \"NaN\", data[var])\n        train = data.loc[data['TRAIN_INDEX'] == 1]\n        test = data.loc[data['TRAIN_INDEX'] == 0]\n        train_grp = train[[var,target]].groupby(var).agg(['count','mean'])\n        test_grp = test[[var,target]].groupby(var).agg(['count','mean'])\n        grp = pd.merge(train_grp, test_grp, how='outer', left_index=True, right_index=True)\n        grp.columns = ['count_train', 'severity_train', 'count_test', 'severity_test']\n        print(grp)\n        \n        count_max = max([max(grp['count_train']),max(grp['count_test'])])\n        sev_max = max([max(grp['severity_train']).max(grp['severity_test'])])\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arrange(len(df_count.index))\n        width = 0.3\n        multiplier = 0 \n        \n        fig, ax1 = plt.subplots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width + multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier += 1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n        \n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], c='blue', label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], c='darkorange', label='severity_test')\n        ax2.legend(loc='upper right')\n    else:\n        data_temp = data.copy()\n        data_temp['DECILE'] = pd.qcut(data_temp[var], 10, labels=False, duplicates='drop')                   \n        temp = data.copy()\n        temp['DECILE'] = pd.qcut(temp[var], 10, duplicates='drop')\n        data_grp = temp[['DECILE', target]].groupby('DECILE').agg(['count','mean']).reset_index()\n        data_grp.columns = [var+'_BOUNDARY', 'count', 'severity']\n        data_grp = data_grp.drop(['count', 'severity'], axis=1)\n        \n        train = data_temp.loc[data_temp['TRAIN_INDEX'] == 1]\n        test = data_temp.loc[data_temp['TRAIN_INDEX'] == 0]\n        \n        train_temp = train.copy()\n        train_grp = train_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        train_grp.columns = ['count_train', 'severity_train']\n        \n        test_temp = test.copy()\n        test_grp = test_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        test_grp.columns = ['count_test', 'severity_test']\n        \n        grp = pd.merge(data_grp, train_grp, how='outer', left_index=true, right_index=True)\n        grp = pd.merge(grp, test_grp, how='outer', left_index=true, right_index=true)\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arrange(len(df_count.index))\n        width = 0.3\n        multiplier = 0\n        \n        fig, ax1 = plt.sublpots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width * multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier +=1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n\n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], label='severity_test')\n        ax2.legend(loc='upper right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Analysis","metadata":{}},{"cell_type":"code","source":"# Execute Analysis, Data Cleaning Functions\noutput = analysis(df_train)\nmissing = data_cleaning(df_train)\nmissing_data = data_cleaning(df_train)\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:42:40.543703Z","iopub.execute_input":"2024-07-09T15:42:40.544436Z","iopub.status.idle":"2024-07-09T15:42:40.939761Z","shell.execute_reply.started":"2024-07-09T15:42:40.544399Z","shell.execute_reply":"2024-07-09T15:42:40.938680Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Project","metadata":{}},{"cell_type":"markdown","source":"### Summary","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Package List","metadata":{}},{"cell_type":"code","source":"#import pkg_resources\n#installed_packages = pkg_resources.working_set\n#installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n#installed_packages_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Additional Packages\nimport io\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, LogisticRegression, SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import LinearSVR\nimport xgboost\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom scipy import stats #Confidence Interval","metadata":{"execution":{"iopub.status.busy":"2024-07-09T16:19:39.223800Z","iopub.execute_input":"2024-07-09T16:19:39.224342Z","iopub.status.idle":"2024-07-09T16:19:40.367200Z","shell.execute_reply.started":"2024-07-09T16:19:39.224304Z","shell.execute_reply":"2024-07-09T16:19:40.366152Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n#df_description = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\", delimiter=' ')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:42:48.478536Z","iopub.execute_input":"2024-07-09T15:42:48.478884Z","iopub.status.idle":"2024-07-09T15:42:48.527341Z","shell.execute_reply.started":"2024-07-09T15:42:48.478854Z","shell.execute_reply":"2024-07-09T15:42:48.526552Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Combine Train/Test","metadata":{}},{"cell_type":"code","source":"df_train['TRAIN_INDEX'] = 1\n#df_train.columns.to_list()\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:47:05.614854Z","iopub.execute_input":"2024-07-09T15:47:05.615888Z","iopub.status.idle":"2024-07-09T15:47:05.623792Z","shell.execute_reply.started":"2024-07-09T15:47:05.615848Z","shell.execute_reply":"2024-07-09T15:47:05.622621Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(1460, 82)"},"metadata":{}}]},{"cell_type":"code","source":"df_test['TRAIN_INDEX'] = 0\n#df_test.columns.to_list()\ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:46:06.893196Z","iopub.execute_input":"2024-07-09T15:46:06.894121Z","iopub.status.idle":"2024-07-09T15:46:06.902071Z","shell.execute_reply.started":"2024-07-09T15:46:06.894082Z","shell.execute_reply":"2024-07-09T15:46:06.900966Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['Id',\n 'MSSubClass',\n 'MSZoning',\n 'LotFrontage',\n 'LotArea',\n 'Street',\n 'Alley',\n 'LotShape',\n 'LandContour',\n 'Utilities',\n 'LotConfig',\n 'LandSlope',\n 'Neighborhood',\n 'Condition1',\n 'Condition2',\n 'BldgType',\n 'HouseStyle',\n 'OverallQual',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'RoofStyle',\n 'RoofMatl',\n 'Exterior1st',\n 'Exterior2nd',\n 'MasVnrType',\n 'MasVnrArea',\n 'ExterQual',\n 'ExterCond',\n 'Foundation',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinSF1',\n 'BsmtFinType2',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n 'Heating',\n 'HeatingQC',\n 'CentralAir',\n 'Electrical',\n '1stFlrSF',\n '2ndFlrSF',\n 'LowQualFinSF',\n 'GrLivArea',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr',\n 'KitchenQual',\n 'TotRmsAbvGrd',\n 'Functional',\n 'Fireplaces',\n 'FireplaceQu',\n 'GarageType',\n 'GarageYrBlt',\n 'GarageFinish',\n 'GarageCars',\n 'GarageArea',\n 'GarageQual',\n 'GarageCond',\n 'PavedDrive',\n 'WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n '3SsnPorch',\n 'ScreenPorch',\n 'PoolArea',\n 'PoolQC',\n 'Fence',\n 'MiscFeature',\n 'MiscVal',\n 'MoSold',\n 'YrSold',\n 'SaleType',\n 'SaleCondition',\n 'TRAIN_INDEX']"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.concat([df_train, df_test], ignore_index=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:44:00.328968Z","iopub.execute_input":"2024-07-09T15:44:00.329374Z","iopub.status.idle":"2024-07-09T15:44:00.341668Z","shell.execute_reply.started":"2024-07-09T15:44:00.329343Z","shell.execute_reply":"2024-07-09T15:44:00.340587Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(2919, 82)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Functions","metadata":{}},{"cell_type":"code","source":"def histogram_numerical_plots(data):\n    data_x = data.copy()\n    # Select numerical columns\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    # Create subset DataFrame with only numerical values\n    data_x_num_cols = data_x[num_cols.columns]\n    data_x_num_cols.hist(bins=25, figsize=(40,30))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pearson's R\ndef corr_matrix(data):\n    data_x = data.copy()\n    # Select numerical columns\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    corr_cols = data_x[num_cols.columns]\n    corr_matrix = corr_cols.corr()\n    corr_matrix = corr_matrix[\"SalePrice\"].sort_values(ascending=False)\n    return corr_matrix","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:53:02.549019Z","iopub.execute_input":"2024-07-09T15:53:02.549421Z","iopub.status.idle":"2024-07-09T15:53:02.555058Z","shell.execute_reply.started":"2024-07-09T15:53:02.549388Z","shell.execute_reply":"2024-07-09T15:53:02.553971Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Code to determine dtypes, missing values, etc...\ndef analysis(read):\n    \n    if len(read) > 0:\n        print(\"PROCESS HAS BEEN STARTED\\n\")\n\n        print(\"DATA SHAPE\")\n        print(\"Observation:\", read.shape[0], \"Column:\", read.shape[1], \"\\n\")\n\n        print(\"EXPLORE MORE ABOUT THE DATA\")\n        if len(read.select_dtypes(\"object\").columns) > 0:\n            print(\"Object Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"object\").columns), \"\\n\", \n                  read.select_dtypes(\"object\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"integer\").columns) > 0:\n            print(\"Integer Variables:\", \"\\n\", \"VVariables:\", \n                  len(read.select_dtypes(\"integer\").columns), \"\\n\", \n                  read.select_dtypes(\"integer\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"float\").columns) > 0:\n            print(\"Float Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"float\").columns), \"\\n\", \n                  read.select_dtypes(\"float\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"bool\").columns) > 0:\n            print(\"Bool Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"bool\").columns), \"\\n\", \n                  read.select_dtypes(\"bool\").columns.tolist(), \"\\n\")\n\n        print(\"IS THERE ANY MISSING VALUE\")\n        print(\" \\n \", np.where(read.isnull().values.any() == False,\"No missing value!\", \"Data includes missing value!\"), \"\\n\")\n\n        buf = io.StringIO()\n        read.info(buf=buf)\n        check = True\n        check = buf.getvalue().split('\\n')[-2].split(\":\")[1].strip()\n        print(\"MEMORY \\n\", check)\n\n    else:\n        print(\"ERROR!\")\n\n    return read","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect Missing Values\ndef data_cleaning(df):\n\n    print(\"*********{} *********\".format('Inspecting missing values'))\n    \n    data = df.isna().sum().reset_index().sort_values(by=0, ascending=False)\n    clean_data = data[data[0] != 0].shape[0]\n    columns = df.shape[1]\n    rows = df.shape[0]\n    data.columns = [\"name\", \"missing appearences\"]\n    data[\"%missing from total\"] = data[data[\"missing appearences\"]!=0][\"missing appearences\"]/rows\n    mis_data = data[data[\"%missing from total\"] > 0.5].shape[0]\n    #drop_data = np.array(data[data[\"%missing from total\"] > 0.5][\"name\"])\n    \n    print(\"{}/{} total missing data in terms of column shape.\".format(clean_data, columns))\n    #print(\"{}/{} columns  will be dropped. name of the drop column is {}\".format(mis_data, columns,drop_data))\n    \n    return data#, drop_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Univariate Analysis (Compare Train/Test Splits)\ndef univariate_eda(data, target, var) -> dict:\n    if data[var].dtype == 'object':\n        data[var] = np.where(data[var].isna(), \"NaN\", data[var])\n        train = data.loc[data['TRAIN_INDEX'] == 1]\n        test = data.loc[data['TRAIN_INDEX'] == 0]\n        train_grp = train[[var,target]].groupby(var).agg(['count','mean'])\n        test_grp = test[[var,target]].groupby(var).agg(['count','mean'])\n        grp = pd.merge(train_grp, test_grp, how='outer', left_index=True, right_index=True)\n        grp.columns = ['count_train', 'severity_train', 'count_test', 'severity_test']\n        print(grp)\n        \n        count_max = max([max(grp['count_train']),max(grp['count_test'])])\n        sev_max = max([max(grp['severity_train']).max(grp['severity_test'])])\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arrange(len(df_count.index))\n        width = 0.3\n        multiplier = 0 \n        \n        fig, ax1 = plt.subplots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width + multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier += 1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n        \n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], c='blue', label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], c='darkorange', label='severity_test')\n        ax2.legend(loc='upper right')\n    else:\n        data_temp = data.copy()\n        data_temp['DECILE'] = pd.qcut(data_temp[var], 10, labels=False, duplicates='drop')                   \n        temp = data.copy()\n        temp['DECILE'] = pd.qcut(temp[var], 10, duplicates='drop')\n        data_grp = temp[['DECILE', target]].groupby('DECILE').agg(['count','mean']).reset_index()\n        data_grp.columns = [var+'_BOUNDARY', 'count', 'severity']\n        data_grp = data_grp.drop(['count', 'severity'], axis=1)\n        \n        train = data_temp.loc[data_temp['TRAIN_INDEX'] == 1]\n        test = data_temp.loc[data_temp['TRAIN_INDEX'] == 0]\n        \n        train_temp = train.copy()\n        train_grp = train_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        train_grp.columns = ['count_train', 'severity_train']\n        \n        test_temp = test.copy()\n        test_grp = test_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        test_grp.columns = ['count_test', 'severity_test']\n        \n        grp = pd.merge(data_grp, train_grp, how='outer', left_index=true, right_index=True)\n        grp = pd.merge(grp, test_grp, how='outer', left_index=true, right_index=true)\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arrange(len(df_count.index))\n        width = 0.3\n        multiplier = 0\n        \n        fig, ax1 = plt.sublpots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width * multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier +=1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n\n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], label='severity_test')\n        ax2.legend(loc='upper right')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Peformance Metrics","metadata":{}},{"cell_type":"code","source":"def evaluation_metrics(actual, pred) -> dict:\n    rmse = np.sqrt(np.mean(pred-actual)**2)\n    me = np.mean(pred-actual)\n    mep = np.mean(pred-actual)/np.mean(actual)\n    return {\"rmse\": Root Mean Squared Error, \"me\": Mean Error, \"mep\": Mean Error Percent}\n\ndef lift_chart(data, pred, act) -> dict:\n    data['Prediction_Decile'] = pd.qcut(data[pred], 20, labels=False, duplicates='drop')\n    lift_chart_prediction = data.groupby('Prediction_Decile')[pred].mean().to_frame(name = 'pred').reset_index()\n    lift_chart_actual = data.groupby('Prediction_Decile')[act].mean().to_frame(name = 'act').reset_index()\n    lift_chart_data = pd.merge(lift_chart_prediction, lift_chart_actual, how='inner', left_on=['Prediction_Decile'], right_on=[])\n    \n    lift_chart_plot = plt.figsize()\n    plt.plot(lift_chart_data['Prediction_Decile'], lift_chart_data['Prediction_Decile'], label = 'pred', color='red')\n    plt.plot(lift_chart_data['Prediction_Decile'], lift_chart_data['Prediction_Decile'], label = 'act', color='black')\n    plt.xlabel(\"Prediction Decile\")\n    plt.ylabel(\"Average Cost\")\n    plt.legend(loc=\"upper right\")\n    plt.gca().set_title(\"Model Performance Lift Chart\")\n    \n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Understanding Outliers\n\n## Cook's Distance\n\n## Leverage\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable Correlation Analysis\n\n## T-Testing\n\n## Chi-Square Test\n\n## P-Value Testing\n\n## One-Way Analysis\n\n## VIF Calculations\n\n## Correlation Matrix\n\n## Principal Component Analysis (PCA)\n\n## \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Exploratory Data Analysis\nhistogram_numerical_plots(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Pearson's R Plot\ncorr_matrix(df)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T15:53:09.009559Z","iopub.execute_input":"2024-07-09T15:53:09.010502Z","iopub.status.idle":"2024-07-09T15:53:09.040178Z","shell.execute_reply.started":"2024-07-09T15:53:09.010468Z","shell.execute_reply":"2024-07-09T15:53:09.039302Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"SalePrice        1.000000\nOverallQual      0.790982\nGrLivArea        0.708624\nGarageCars       0.640409\nGarageArea       0.623431\nTotalBsmtSF      0.613581\n1stFlrSF         0.605852\nFullBath         0.560664\nTotRmsAbvGrd     0.533723\nYearBuilt        0.522897\nYearRemodAdd     0.507101\nGarageYrBlt      0.486362\nMasVnrArea       0.477493\nFireplaces       0.466929\nBsmtFinSF1       0.386420\nLotFrontage      0.351799\nWoodDeckSF       0.324413\n2ndFlrSF         0.319334\nOpenPorchSF      0.315856\nHalfBath         0.284108\nLotArea          0.263843\nBsmtFullBath     0.227122\nBsmtUnfSF        0.214479\nBedroomAbvGr     0.168213\nScreenPorch      0.111447\nPoolArea         0.092404\nMoSold           0.046432\n3SsnPorch        0.044584\nBsmtFinSF2      -0.011378\nBsmtHalfBath    -0.016844\nMiscVal         -0.021190\nId              -0.021917\nLowQualFinSF    -0.025606\nYrSold          -0.028923\nOverallCond     -0.077856\nMSSubClass      -0.084284\nEnclosedPorch   -0.128578\nKitchenAbvGr    -0.135907\nTRAIN_INDEX           NaN\nName: SalePrice, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Categorical Variables","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical Variables","metadata":{}},{"cell_type":"code","source":"### Imputation Strategies\nfrom sklearn.impute import SimpleImputer\n\ndef mean_imputation(data):\n    data_x = data.copy()\n    mean_imputer = SimpleImputer(strategy=\"median\")\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    # Create subset DataFrame with only numerical values\n    data_imputation = data_x[num_cols.columns]\n    data_x\n    \ndef log_transformation(data):\n    \ndef sqrt_transformation(data):\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Modeling Functions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Train/Test Split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step-Wise\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GLM Models\n\n## Gamma\n\n## Inverse Gaussian\n\n## Logistic/Binomial\n\n## Multinomial\n\n## Normal\n\n## Poisson\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree (CART)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lasso Model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elastic Net\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting Machine\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machine\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Analysis","metadata":{}},{"cell_type":"code","source":"# Execute Analysis, Data Cleaning Functions\noutput = analysis(df_train)\nmissing = data_cleaning(df_train)\nmissing_data = data_cleaning(df_train)\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
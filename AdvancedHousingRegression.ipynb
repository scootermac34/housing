{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Project","metadata":{}},{"cell_type":"markdown","source":"### Summary","metadata":{}},{"cell_type":"markdown","source":"### Package List","metadata":{}},{"cell_type":"code","source":"#import pkg_resources\n#installed_packages = pkg_resources.working_set\n#installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n#installed_packages_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Additional Packages\nimport io\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, LogisticRegression, SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import LinearSVR\nimport xgboost\n\nfrom scipy import stats #Confidence Interval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the column names\ncolumns_df1 = df_train.columns\ncolumns_df2 = df_test.columns\n\n# Find the missing column\nmissing_in_df1 = columns_df2[~columns_df2.isin(columns_df1)]\nmissing_in_df2 = columns_df1[~columns_df1.isin(columns_df2)]\n\nif not missing_in_df1.empty:\n    print(f\"Columns missing in df1: {missing_in_df1}\")\nif not missing_in_df2.empty:\n    print(f\"Columns missing in df2: {missing_in_df2}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combine Train/Test","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf_train['TRAIN_INDEX'] = 1\n#df_train.columns.to_list()\n#df_train.shape\n\n# Test Data\ndf_test['TRAIN_INDEX'] = 0\n#df_test.columns.to_list()\n#df_test.shape\n\n# Combine Train/Test\ndf = pd.concat([df_train, df_test], ignore_index=True)\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis Functions","metadata":{}},{"cell_type":"code","source":"# Code to determine dtypes, missing values, etc...\ndef analysis(read):\n    \n    if len(read) > 0:\n        print(\"PROCESS HAS BEEN STARTED\\n\")\n\n        print(\"DATA SHAPE\")\n        print(\"Observation:\", read.shape[0], \"Column:\", read.shape[1], \"\\n\")\n\n        print(\"EXPLORE MORE ABOUT THE DATA\")\n        if len(read.select_dtypes(\"object\").columns) > 0:\n            print(\"Object Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"object\").columns), \"\\n\", \n                  read.select_dtypes(\"object\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"integer\").columns) > 0:\n            print(\"Integer Variables:\", \"\\n\", \"VVariables:\", \n                  len(read.select_dtypes(\"integer\").columns), \"\\n\", \n                  read.select_dtypes(\"integer\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"float\").columns) > 0:\n            print(\"Float Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"float\").columns), \"\\n\", \n                  read.select_dtypes(\"float\").columns.tolist(), \"\\n\")\n\n        if len(read.select_dtypes(\"bool\").columns) > 0:\n            print(\"Bool Variables:\", \"\\n\", \"Variables:\", \n                  len(read.select_dtypes(\"bool\").columns), \"\\n\", \n                  read.select_dtypes(\"bool\").columns.tolist(), \"\\n\")\n\n        print(\"IS THERE ANY MISSING VALUE\")\n        print(\" \\n \", np.where(read.isnull().values.any() == False,\"No missing value!\", \"Data includes missing value!\"), \"\\n\")\n\n        buf = io.StringIO()\n        read.info(buf=buf)\n        check = True\n        check = buf.getvalue().split('\\n')[-2].split(\":\")[1].strip()\n        print(\"MEMORY \\n\", check)\n\n    else:\n        print(\"ERROR!\")\n\n    return read","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect Missing Values\ndef data_cleaning(df):\n\n    print(\"*********{} *********\".format('Inspecting missing values'))\n    \n    data = df.isna().sum().reset_index().sort_values(by=0, ascending=False)\n    clean_data = data[data[0] != 0].shape[0]\n    columns = df.shape[1]\n    rows = df.shape[0]\n    data.columns = [\"name\", \"missing appearences\"]\n    data[\"%missing from total\"] = data[data[\"missing appearences\"]!=0][\"missing appearences\"]/rows\n    mis_data = data[data[\"%missing from total\"] > 0.5].shape[0]\n    #drop_data = np.array(data[data[\"%missing from total\"] > 0.5][\"name\"])\n    \n    print(\"{}/{} total missing data in terms of column shape.\".format(clean_data, columns))\n    #print(\"{}/{} columns  will be dropped. name of the drop column is {}\".format(mis_data, columns,drop_data))\n    \n    return data#, drop_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def histogram_numerical_plots(data):\n    data_x = data.copy()\n    # Select numerical columns\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    # Create subset DataFrame with only numerical values\n    data_x_num_cols = data_x[num_cols.columns]\n    data_x_num_cols.hist(bins=25, figsize=(40,30))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pearson's R\ndef corr_matrix(data):\n    data_x = data.copy()\n    # Select numerical columns\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    corr_cols = data_x[num_cols.columns]\n    corr_matrix = corr_cols.corr()\n    corr_matrix = corr_matrix[\"SalePrice\"].sort_values(ascending=False)\n    return corr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['TRAIN_INDEX'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Univariate Analysis (Compare Train/Test Splits)\ndef univariate_eda(data, target, var) -> dict:\n    if data[var].dtype == 'object':\n        data[var] = np.where(data[var].isna(), \"NaN\", data[var])\n        train = data.loc[data['TRAIN_INDEX'] == 1]\n        test = data.loc[data['TRAIN_INDEX'] == 0]\n        train_grp = train[[var,target]].groupby(var).agg(['count','mean'])\n        test_grp = test[[var,target]].groupby(var).agg(['count','mean'])\n        grp = pd.merge(train_grp, test_grp, how='outer', left_index=True, right_index=True)\n        grp.columns = ['count_train', 'severity_train', 'count_test', 'severity_test']\n        print(grp)\n        \n        count_max = max([max(grp['count_train']),max(grp['count_test'])])\n        sev_max = max([max(grp['severity_train']),max(grp['severity_test'])])\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arange(len(df_count.index))\n        width = 0.3\n        multiplier = 0 \n        \n        fig, ax1 = plt.subplots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width + multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier += 1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n        \n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], c='blue', label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], c='darkorange', label='severity_test')\n        ax2.legend(loc='upper right')\n    else:\n        data_temp = data.copy()\n        data_temp['DECILE'] = pd.qcut(data_temp[var], 10, labels=False, duplicates='drop')                   \n        temp = data.copy()\n        temp['DECILE'] = pd.qcut(temp[var], 10, duplicates='drop')\n        data_grp = temp[['DECILE', target]].groupby('DECILE').agg(['count','mean']).reset_index()\n        data_grp.columns = [var+'_BOUNDARY', 'count', 'severity']\n        data_grp = data_grp.drop(['count', 'severity'], axis=1)\n        \n        train = data_temp.loc[data_temp['TRAIN_INDEX'] == 1]\n        test = data_temp.loc[data_temp['TRAIN_INDEX'] == 0]\n        \n        train_temp = train.copy()\n        train_grp = train_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        train_grp.columns = ['count_train', 'severity_train']\n        \n        test_temp = test.copy()\n        test_grp = test_temp[['DECILE', target]].groupby('DECILE').agg(['count','mean'])\n        test_grp.columns = ['count_test', 'severity_test']\n        \n        grp = pd.merge(data_grp, train_grp, how='outer', left_index=True, right_index=True)\n        grp = pd.merge(grp, test_grp, how='outer', left_index=True, right_index=True)\n        \n        df_count = grp[['count_train', 'count_test']].copy()\n        df_sev = grp[['severity_train', 'severity_test']].copy()\n        \n        x = np.arange(len(df_count.index))\n        width = 0.3\n        multiplier = 0\n        \n        fig, ax1 = plt.subplots(constrained_layout=True)\n        \n        for attribute, measurement in df_count.items():\n            offset = width * multiplier\n            rects = ax1.bar(x + offset, measurement, width, label=attribute)\n            multiplier +=1\n            \n        ax1.set_title('numerical: ' + var)\n        \n        ax1.set_ylabel('Counts')\n        ax1.set_xticks(x)\n        ax1.set_xticklabels(df_count.index, rotation=90)\n        ax1.legend(loc='upper left')\n\n        ax2 = ax1.twinx()\n        ax2.set_ylabel('Severity')\n        ax2.plot(df_sev.index, df_sev['severity_train'], label='severity_train')\n        ax2.plot(df_sev.index, df_sev['severity_test'], label='severity_test')\n        ax2.legend(loc='upper right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling Performance Functions","metadata":{}},{"cell_type":"code","source":"def evaluation_metrics(actual, pred) -> dict:\n    rmse = np.sqrt(np.mean(pred-actual)**2)\n    me = np.mean(pred-actual)\n    mep = np.mean(pred-actual)/np.mean(actual)\n    return {\"rmse\": Root Mean Squared Error, \"me\": Mean Error, \"mep\": Mean Error Percent}\n\ndef lift_chart(data, pred, act) -> dict:\n    data['Prediction_Decile'] = pd.qcut(data[pred], 20, labels=False, duplicates='drop')\n    lift_chart_prediction = data.groupby('Prediction_Decile')[pred].mean().to_frame(name = 'pred').reset_index()\n    lift_chart_actual = data.groupby('Prediction_Decile')[act].mean().to_frame(name = 'act').reset_index()\n    lift_chart_data = pd.merge(lift_chart_prediction, lift_chart_actual, how='inner', left_on=['Prediction_Decile'], right_on=[])\n    \n    lift_chart_plot = plt.figsize()\n    plt.plot(lift_chart_data['Prediction_Decile'], lift_chart_data['Prediction_Decile'], label = 'pred', color='red')\n    plt.plot(lift_chart_data['Prediction_Decile'], lift_chart_data['Prediction_Decile'], label = 'act', color='black')\n    plt.xlabel(\"Prediction Decile\")\n    plt.ylabel(\"Average Cost\")\n    plt.legend(loc=\"upper right\")\n    plt.gca().set_title(\"Model Performance Lift Chart\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Understanding Outliers\n\n## Cook's Distance\n\n## Leverage\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variable Correlation Analysis\n\n## T-Testing\n\n## Chi-Square Test\n\nfrom scipy.stats import chi2_contingency\n\ndef chi_square_test(df, col1, col2):\n    \"\"\"\n    Perform chi-square test of independence between two categorical variables in a DataFrame.\n\n    Parameters:\n    df (DataFrame): Pandas DataFrame containing the categorical variables.\n    col1 (str): Name of the first categorical column.\n    col2 (str): Name of the second categorical column.\n\n    Returns:\n    chi2_stat (float): The test statistic of the chi-square test.\n    p_val (float): The p-value of the test.\n    dof (int): Degrees of freedom.\n    contingency_table (DataFrame): The contingency table showing the observed frequencies.\n    \"\"\"\n\n    # Create contingency table\n    contingency_table = pd.crosstab(df[col1], df[col2])\n    \n    # Perform chi-square test\n    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n    \n    return chi2_stat, p_val, dof, contingency_table\n\n## P-Value Testing\n\n## One-Way Analysis\n\n## VIF Calculations\n\n## Correlation Matrix\n\n## Principal Component Analysis (PCA)\n\n## \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Execute Analysis, Data Cleaning Functions\noutput = analysis(df_train)\nmissing = data_cleaning(df_train)\nmissing_data = data_cleaning(df_train)\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Exploratory Data Analysis\nhistogram_numerical_plots(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Pearson's R Plot\ncorr_matrix(df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Categorical Variables","metadata":{}},{"cell_type":"code","source":"categorical_variables = ['MSZoning', 'Street', 'Alley', 'LotShape'] #, 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', \n                         #'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n                         #'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                         #'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', \n                         #'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                         #'SaleType', 'SaleCondition']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['LotShape'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_variables = ['LotShape']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for var in categorical_variables:\n    univariate_eda(df, 'SalePrice', var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical Variables","metadata":{}},{"cell_type":"code","source":"### Imputation Strategies\nfrom sklearn.impute import SimpleImputer\n\ndef mean_imputation(data):\n    data_x = data.copy()\n    mean_imputer = SimpleImputer(strategy=\"median\")\n    num_cols = data_x.select_dtypes(include=['int64', 'float64'])\n    # Create subset DataFrame with only numerical values\n    data_imputation = data_x[num_cols.columns]\n    data_x\n    \ndef log_transformation(data):\n    \ndef sqrt_transformation(data):\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Transformer","metadata":{}},{"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, selected_features):\n        self.selected_features = selected_features\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.selected_features]\n\n# Define selected features (numerical and categorical)\nnumerical_features = ['num_feature1', 'num_feature2']\ncategorical_features = ['cat_feature1', 'cat_feature2']\n\n# Create pipelines for numerical and categorical preprocessing\nnumerical_pipeline = Pipeline([\n    ('num_selector', FeatureSelector(numerical_features)),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline([\n    ('cat_selector', FeatureSelector(categorical_features)),\n    ('encoder', OneHotEncoder())\n])\n\n# Combine numerical and categorical pipelines\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Pipelines","metadata":{}},{"cell_type":"code","source":"## Step-Wise\n\n## Tweedie\nglm_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('glm', LinearRegression())  # Replace with your GLM model\n])\n\n## Inverse Gaussian\n\n## Logistic/Binomial\n\n## Multinomial\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GLM pipeline\n\n\n# Lasso pipeline\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train/Test Split","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_data = df[df['TRAIN_IND'] == 1].copy()\ntest_data = df[df['TRAIN_IND'] == 0].copy()\n\nX_train = train_data.drop(['TRAIN_IND', 'target_column'], axis=1)  # Drop 'TRAIN_IND' and target column\ny_train = train_data['target_column']  # Replace 'target_column' with your actual target column\n\nX_test = test_data.drop(['TRAIN_IND', 'target_column'], axis=1)  # Drop 'TRAIN_IND' and target column\ny_test = test_data['target_column']  # Replace 'target_column' with your actual target column","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Modeling Functions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Naive Model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree (CART)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lasso Model\nlasso_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('lasso', Lasso(alpha=0.1))  # Replace with your Lasso model and set alpha\n])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elastic Net\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Boosting Machine\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Machine\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Neural Network\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Fitting","metadata":{}},{"cell_type":"code","source":"# Fit GLM pipeline\nglm_pipeline.fit(X_train, y_train)\n\n# Evaluate GLM pipeline\nglm_score = glm_pipeline.score(X_test, y_test)\nprint(f\"GLM Model R^2 Score: {glm_score}\")\n\n# Fit Lasso pipeline\nlasso_pipeline.fit(X_train, y_train)\n\n# Evaluate Lasso pipeline\nlasso_score = lasso_pipeline.score(X_test, y_test)\nprint(f\"Lasso Model R^2 Score: {lasso_score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Analysis","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}